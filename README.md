# é­”æ­å¹³å°å¤§è¯­è¨€æ¨¡å‹éƒ¨ç½²ä¸å¯¹æ¯”æµ‹è¯•é¡¹ç›®

## é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ—¨åœ¨é€šè¿‡é­”æ­å¹³å°ï¼ˆModelScopeï¼‰éƒ¨ç½²å’Œå¯¹æ¯”æµ‹è¯•ä¸‰ä¸ªä¸»æµå¤§è¯­è¨€æ¨¡å‹ï¼ŒåŒ…æ‹¬ï¼š
- **é€šä¹‰åƒé—®Qwen-7B-Chat**
- **æ™ºè°±ChatGLM3-6B** 
- **ç™¾å·2-7B-å¯¹è¯æ¨¡å‹**

é€šè¿‡æ ‡å‡†åŒ–çš„æµ‹è¯•æ¡ˆä¾‹ï¼Œå¯¹è¿™äº›æ¨¡å‹çš„è¯­è¨€ç†è§£èƒ½åŠ›ã€å¯¹è¯è´¨é‡å’Œæ¨ç†æ€§èƒ½è¿›è¡Œæ¨ªå‘å¯¹æ¯”åˆ†æã€‚

## ç¯å¢ƒå‡†å¤‡

### 1. æ³¨å†Œå¹¶ç™»å½•é­”æ­å¹³å°

1. è®¿é—®é­”æ­å¹³å°å®˜ç½‘ï¼šhttps://modelscope.cn/
2. ç‚¹å‡»å³ä¸Šè§’"ç™»å½•"æŒ‰é’®
3. é€‰æ‹©"é˜¿é‡Œäº‘è´¦å·ç™»å½•"
   
### 2. åˆ›å»ºNotebookç¯å¢ƒ

1. åœ¨é­”æ­å¹³å°ä¸»é¡µï¼Œç‚¹å‡»"åˆ›å»ºç©ºé—´"
2.  ç‚¹å‡»"åˆ›å»º"å¹¶ç­‰å¾…ç¯å¢ƒå¯åŠ¨

## æ¨¡å‹éƒ¨ç½²æµç¨‹

### 1. ç¯å¢ƒé…ç½®

åœ¨Jupyter Notebookä¸­æ‰§è¡Œä»¥ä¸‹ä»£ç å®‰è£…å¿…è¦ä¾èµ–ï¼š

```python
# å®‰è£…ModelScope SDK
!pip install modelscope

# å®‰è£…å…¶ä»–å¿…è¦ä¾èµ–
!pip install torch transformers tokenizers
!pip install pandas numpy matplotlib seaborn
!pip install jupyter notebook
```

### 2. æ¨¡å‹ä¸‹è½½ä¸åŠ è½½

#### 2.1 é€šä¹‰åƒé—®Qwen-7B-Chat

```python
from transformers import TextStreamer, AutoTokenizer, AutoModelForCausalLM

# æ¨¡å‹æœ¬åœ°è·¯å¾„
model_name = "/mnt/data/Qwen-7B-Chat"

# è¾“å…¥æç¤º
prompt = "What's the difference between these two sentences?\n"
    "1. He said she said he wouldn't say.\n"
    "2. She said he said she wouldn't say."

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    trust_remote_code=True
)

# åŠ è½½æ¨¡å‹å¹¶è®¾ä¸ºè¯„ä¼°æ¨¡å¼
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    trust_remote_code=True,
    torch_dtype="auto"  # è‡ªåŠ¨é€‰æ‹© float32 æˆ– float16ï¼Œä¾æ®æ¨¡å‹é…ç½®
).eval()

# ç¼–ç è¾“å…¥
inputs = tokenizer(prompt, return_tensors="pt").input_ids
)
# æ¨ç†ç”Ÿæˆ
outputs = model.generate(inputs, max_new_tokens=100)
response = tokenizer.decode(outputs[0], skip_special_tokens=True)

print(response)
```

#### 2.2 ChatGLM3-6B æœ¬åœ°éƒ¨ç½²
# ğŸ§± ç¯å¢ƒå‡†å¤‡

### âœ… 1. åˆ›å»º Conda è™šæ‹Ÿç¯å¢ƒ

```bash
conda create -n glm_env python=3.10 -y
conda activate glm_env
```

### âœ… 2. å®‰è£… PyTorch + torchvision ï¼ˆCPU ç‰ˆæœ¬ï¼Œ2.6.0 + 0.17.0ï¼‰

```bash
pip install torch==2.6.0 torchvision==0.17.0 --index-url https://download.pytorch.org/whl/cpu
```

---

## ğŸ“¦ å®‰è£…ä¾èµ–

```bash
pip install transformers==4.33.3
pip install sentencepiece accelerate tqdm
pip install modelscope
```

> è¯´æ˜ï¼š`transformers==4.33.3` æ˜¯ ChatGLM3 å®˜æ–¹æµ‹è¯•é«˜ç¬¦ç‰ˆæœ¬ï¼Œ`modelscope` ç”¨äºæ¨¡å‹è‡ªåŠ¨ä¸‹è½½

---

## â¬‡ï¸ ä¸‹è½½ ChatGLM3-6B æ¨¡å‹

> å¯ä½¿ç”¨ git clone ä¸‹è½½ä»£ç ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ `snapshot_download()` è‡ªåŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°

```bash
git clone https://www.modelscope.cn/ZhipuAI/chatglm3-6b.git
cd chatglm3-6b
```

---

## ğŸš€ å¯åŠ¨æ¨¡å‹ (CPU)

ä¿å­˜ä¸º `run.py`ï¼š

```python
from modelscope import AutoTokenizer, AutoModel, snapshot_download

model_dir = snapshot_download("ZhipuAI/chatglm3-6b", revision="v1.0.0")
tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
model = AutoModel.from_pretrained(model_dir, trust_remote_code=True).float().eval()  # CPU ä¸Šè¿è¡Œ

response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
print("Bot:", response)

response, history = model.chat(tokenizer, "æ™šä¸Šç¡ä¸ç€æ€ä¹ˆåŠï¼Ÿ", history=history)
print("Bot:", response)
```

æ‰§è¡Œï¼š
```bash
python run.py
```

---

```python
from modelscope import AutoTokenizer, AutoModel, snapshot_download

# ä¸‹è½½æ¨¡å‹ï¼ˆé¦–æ¬¡ä½¿ç”¨æ—¶æ‰§è¡Œï¼Œæ¨¡å‹ä¼šç¼“å­˜è‡³ ~/.cache/modelscope/hub/ï¼‰
model_dir = snapshot_download("ZhipuAI/chatglm3-6b", revision="v1.0.0")

# åŠ è½½ tokenizer
tokenizer = AutoTokenizer.from_pretrained(
    model_dir,
    trust_remote_code=True
)

# åŠ è½½æ¨¡å‹å¹¶è½¬æ¢ä¸º float32ï¼ˆé¿å…åœ¨ CPU ä¸Šä½¿ç”¨ half ç²¾åº¦å¯¼è‡´æŠ¥é”™ï¼‰
model = AutoModel.from_pretrained(
    model_dir,
    trust_remote_code=True
).float()

# è®¾ç½®æ¨¡å‹ä¸ºæ¨ç†æ¨¡å¼
model = model.eval()

# å•è½®å¯¹è¯æµ‹è¯•
response, history = model.chat(
    tokenizer,
    "What's the difference between these two sentences?\n"
    "1. He said she said he wouldn't say.\n"
    "2. She said he said she wouldn't say.",
    history=[]
)

# è¾“å‡ºç»“æœ
print(response)
```

#### 2.3 ç™¾å·2-7B-å¯¹è¯æ¨¡å‹

```python
# ä¸‹è½½å¹¶åŠ è½½ç™¾å·2-7B-Chatæ¨¡å‹
model_name_baichuan = "baichuan-inc/Baichuan2-7B-Chat"
tokenizer_baichuan = AutoTokenizer.from_pretrained(model_name_baichuan, trust_remote_code=True)
model_baichuan = AutoModelForCausalLM.from_pretrained(
    model_name_baichuan,
    trust_remote_code=True,
    torch_dtype=torch.float16,
    device_map="auto"
)
```

### 3. æ¨¡å‹æ¨ç†å‡½æ•°

```python
def generate_response(model, tokenizer, prompt, max_length=2048):
    """
    é€šç”¨çš„æ¨¡å‹æ¨ç†å‡½æ•°
    """
    inputs = tokenizer.encode(prompt, return_tensors="pt")
    
    with torch.no_grad():
        outputs = model.generate(
            inputs,
            max_length=max_length,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response.replace(prompt, "").strip()
```

## æµ‹è¯•æ¡ˆä¾‹è®¾è®¡

### æµ‹è¯•é—®é¢˜é›†

æˆ‘ä»¬è®¾è®¡äº†ä»¥ä¸‹æµ‹è¯•é—®é¢˜æ¥è¯„ä¼°æ¨¡å‹çš„è¯­è¨€ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼š

```python
test_questions = [
    {
        "id": 1,
        "question": "è¯·è¯´å‡ºä»¥ä¸‹ä¸¤å¥è¯åŒºåˆ«åœ¨å“ªé‡Œï¼Ÿ\n1ã€å†¬å¤©ï¼šèƒ½ç©¿å¤šå°‘ç©¿å¤šå°‘\n2ã€å¤å¤©ï¼šèƒ½ç©¿å¤šå°‘ç©¿å¤šå°‘",
        "category": "è¯­è¨€ç†è§£-æ­§ä¹‰æ¶ˆè§£",
        "language": "ä¸­æ–‡"
    },
    {
        "id": 2,
        "question": "è¯·è¯´å‡ºä»¥ä¸‹ä¸¤å¥è¯åŒºåˆ«åœ¨å“ªé‡Œï¼Ÿå•èº«ç‹—äº§ç”Ÿçš„åŸå› æœ‰ä¸¤ä¸ªï¼Œä¸€æ˜¯è°éƒ½çœ‹ä¸ä¸Šï¼ŒäºŒæ˜¯è°éƒ½çœ‹ä¸ä¸Šã€‚"
        "category": "è¯­è¨€ç†è§£-å¥æ³•åˆ†æ",
        "language": "ä¸­æ–‡"
    },
    {
        "id": 3,
        "question": "What's the difference between these two sentences?\n1. He said she said he wouldn't say.\n2. She said he said she wouldn't say.",
        "category": "è¯­è¨€ç†è§£-å¥æ³•åˆ†æ",
        "language": "è‹±æ–‡"
    },
]
```

### æµ‹è¯•æ‰§è¡Œä»£ç 

```python

   
```

## ç»“æœåˆ†æä¸å¯¹æ¯”

### 1. åŸºç¡€ç»Ÿè®¡åˆ†æ

```python
# æ˜¾ç¤ºåŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
print("=== æµ‹è¯•ç»“æœç»Ÿè®¡ ===")
print(f"æ€»æµ‹è¯•æ•°é‡: {len(results_df)}")
print(f"æˆåŠŸæµ‹è¯•æ•°é‡: {len(results_df[results_df['status'] == 'success'])}")
print(f"å¤±è´¥æµ‹è¯•æ•°é‡: {len(results_df[results_df['status'] == 'error'])}")

# æŒ‰æ¨¡å‹åˆ†ç»„ç»Ÿè®¡
model_stats = results_df.groupby('model').agg({
    'response_time': ['mean', 'median', 'std'],
    'status': lambda x: (x == 'success').sum()
}).round(3)

print("\n=== å„æ¨¡å‹æ€§èƒ½ç»Ÿè®¡ ===")
print(model_stats)
```

### 2. å“åº”æ—¶é—´å¯¹æ¯”

```python
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# å“åº”æ—¶é—´å¯¹æ¯”å›¾
plt.figure(figsize=(12, 6))
sns.boxplot(data=results_df[results_df['status'] == 'success'], 
            x='model', y='response_time')
plt.title('å„æ¨¡å‹å“åº”æ—¶é—´å¯¹æ¯”')
plt.ylabel('å“åº”æ—¶é—´ (ç§’)')
plt.xlabel('æ¨¡å‹')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```

### 3. è¯¦ç»†ç»“æœå±•ç¤º

```python
# æŒ‰é—®é¢˜åˆ†ç±»å±•ç¤ºç»“æœ
for question_id in sorted(results_df['question_id'].unique()):
    question_data = results_df[results_df['question_id'] == question_id]
    
    print(f"\n{'='*80}")
    print(f"é—®é¢˜ {question_id}: {question_data.iloc[0]['category']}")
    print(f"{'='*80}")
    print(f"é—®é¢˜å†…å®¹: {question_data.iloc[0]['question']}")
    print(f"\nå„æ¨¡å‹å›ç­”å¯¹æ¯”:")
    
    for _, row in question_data.iterrows():
        print(f"\nã€{row['model']}ã€‘")
        print(f"å›ç­”: {row['response']}")
        print(f"å“åº”æ—¶é—´: {row['response_time']:.2f}ç§’")
        print(f"çŠ¶æ€: {row['status']}")
        print("-" * 60)
```

## æ¨¡å‹èƒ½åŠ›å¯¹æ¯”åˆ†æ

### 1. è¯­è¨€ç†è§£èƒ½åŠ›

åŸºäºæµ‹è¯•ç»“æœï¼Œæˆ‘ä»¬å¯ä»¥ä»ä»¥ä¸‹ç»´åº¦å¯¹æ¯”å„æ¨¡å‹ï¼š

#### ä¸­æ–‡ç†è§£èƒ½åŠ›
- **æ­§ä¹‰æ¶ˆè§£**ï¼šæµ‹è¯•æ¨¡å‹å¯¹"èƒ½ç©¿å¤šå°‘ç©¿å¤šå°‘"åœ¨ä¸åŒå­£èŠ‚è¯­å¢ƒä¸‹çš„ç†è§£
- **å¥æ³•åˆ†æ**ï¼šæµ‹è¯•æ¨¡å‹å¯¹å¤æ‚ä¸­æ–‡å¥å¼çš„è§£æèƒ½åŠ›

#### è‹±æ–‡ç†è§£èƒ½åŠ›
- **è¯­æ³•ç»“æ„**ï¼šæµ‹è¯•æ¨¡å‹å¯¹è‹±æ–‡å¤æ‚å¥å¼çš„ç†è§£
- **è¯­ä¹‰æ­§ä¹‰**ï¼šæµ‹è¯•æ¨¡å‹å¯¹åŒå½¢å¼‚ä¹‰å¥å­çš„åŒºåˆ†èƒ½åŠ›

### 2. æ¨ç†é€»è¾‘èƒ½åŠ›

```python
def analyze_reasoning_ability(results_df):
    """
    åˆ†æå„æ¨¡å‹çš„æ¨ç†èƒ½åŠ›
    """
    reasoning_analysis = {}
    
    for model in results_df['model'].unique():
        model_data = results_df[results_df['model'] == model]
        
        # è®¡ç®—å„ç±»åˆ«çš„æˆåŠŸç‡
        category_success = model_data.groupby('category').apply(
            lambda x: (x['status'] == 'success').sum() / len(x)
        )
        
        reasoning_analysis[model] = {
            'è¯­è¨€ç†è§£-æ­§ä¹‰æ¶ˆè§£': category_success.get('è¯­è¨€ç†è§£-æ­§ä¹‰æ¶ˆè§£', 0),
            'è¯­è¨€ç†è§£-å¥æ³•åˆ†æ': category_success.get('è¯­è¨€ç†è§£-å¥æ³•åˆ†æ', 0),
            'å¹³å‡å“åº”æ—¶é—´': model_data['response_time'].mean(),
            'æ€»ä½“æˆåŠŸç‡': (model_data['status'] == 'success').sum() / len(model_data)
        }
    
    return reasoning_analysis

reasoning_results = analyze_reasoning_ability(results_df)
```

### 3. ç»¼åˆè¯„ä¼°çŸ©é˜µ

```python
def create_evaluation_matrix(reasoning_results):
    """
    åˆ›å»ºç»¼åˆè¯„ä¼°çŸ©é˜µ
    """
    eval_df = pd.DataFrame(reasoning_results).T
    
    # æ·»åŠ ç»¼åˆè¯„åˆ†ï¼ˆåŸºäºå¤šä¸ªæŒ‡æ ‡çš„åŠ æƒå¹³å‡ï¼‰
    eval_df['ç»¼åˆè¯„åˆ†'] = (
        eval_df['è¯­è¨€ç†è§£-æ­§ä¹‰æ¶ˆè§£'] * 0.3 +
        eval_df['è¯­è¨€ç†è§£-å¥æ³•åˆ†æ'] * 0.3 +
        eval_df['æ€»ä½“æˆåŠŸç‡'] * 0.4
    ) * 100
    
    # å“åº”æ—¶é—´è¯„åˆ†ï¼ˆæ—¶é—´è¶ŠçŸ­åˆ†æ•°è¶Šé«˜ï¼‰
    max_time = eval_df['å¹³å‡å“åº”æ—¶é—´'].max()
    eval_df['å“åº”é€Ÿåº¦è¯„åˆ†'] = (max_time - eval_df['å¹³å‡å“åº”æ—¶é—´']) / max_time * 100
    
    return eval_df.round(2)

evaluation_matrix = create_evaluation_matrix(reasoning_results)
print("=== ç»¼åˆè¯„ä¼°çŸ©é˜µ ===")
print(evaluation_matrix)
```

## ç»“è®ºä¸å»ºè®®

### ä¸»è¦å‘ç°

1. **æ¨¡å‹æ€§èƒ½å·®å¼‚**ï¼š
   - å„æ¨¡å‹åœ¨ä¸åŒç±»å‹çš„è¯­è¨€ç†è§£ä»»åŠ¡ä¸Šè¡¨ç°å­˜åœ¨å·®å¼‚
   - å“åº”æ—¶é—´ä¸æ¨¡å‹å¤æ‚åº¦ç›¸å…³

2. **åº”ç”¨åœºæ™¯é€‚é…**ï¼š
   - ä¸­æ–‡åœºæ™¯ï¼š[æ ¹æ®å®é™…æµ‹è¯•ç»“æœå¡«å†™]
   - è‹±æ–‡åœºæ™¯ï¼š[æ ¹æ®å®é™…æµ‹è¯•ç»“æœå¡«å†™]
   - æ¨ç†ä»»åŠ¡ï¼š[æ ¹æ®å®é™…æµ‹è¯•ç»“æœå¡«å†™]

### ä½¿ç”¨å»ºè®®

1. **é€‰æ‹©æ ‡å‡†**ï¼š
   - å¦‚æœæ³¨é‡å“åº”é€Ÿåº¦ï¼šé€‰æ‹©å“åº”æ—¶é—´æœ€çŸ­çš„æ¨¡å‹
   - å¦‚æœæ³¨é‡å‡†ç¡®æ€§ï¼šé€‰æ‹©ç»¼åˆè¯„åˆ†æœ€é«˜çš„æ¨¡å‹
   - å¦‚æœæ³¨é‡ä¸­æ–‡ç†è§£ï¼šé€‰æ‹©ä¸­æ–‡æµ‹è¯•è¡¨ç°æœ€ä½³çš„æ¨¡å‹

2. **éƒ¨ç½²å»ºè®®**ï¼š
   - ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨æˆåŠŸç‡æœ€é«˜çš„æ¨¡å‹
   - å¼€å‘æµ‹è¯•å¯ä»¥ä½¿ç”¨å“åº”æœ€å¿«çš„æ¨¡å‹
   - ç‰¹å®šåœºæ™¯å¯ä»¥æ ¹æ®ä¸“é¡¹æµ‹è¯•ç»“æœé€‰æ‹©

## é¡¹ç›®æ–‡ä»¶ç»“æ„

```
hw4/
â”œâ”€â”€ README.md                 # é¡¹ç›®è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ model_deployment.ipynb   # æ¨¡å‹éƒ¨ç½²ä»£ç 
â”‚   â”œâ”€â”€ testing_framework.ipynb # æµ‹è¯•æ¡†æ¶ä»£ç 
â”‚   â””â”€â”€ results_analysis.ipynb  # ç»“æœåˆ†æä»£ç 
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ test_results.csv         # æµ‹è¯•ç»“æœæ•°æ®
â”‚   â”œâ”€â”€ evaluation_matrix.csv    # è¯„ä¼°çŸ©é˜µ
â”‚   â””â”€â”€ performance_charts/      # æ€§èƒ½å¯¹æ¯”å›¾è¡¨
â”œâ”€â”€ data/
â”‚   â””â”€â”€ test_questions.json      # æµ‹è¯•é—®é¢˜é›†
â””â”€â”€ utils/
    â”œâ”€â”€ model_utils.py          # æ¨¡å‹å·¥å…·å‡½æ•°
    â””â”€â”€ analysis_utils.py       # åˆ†æå·¥å…·å‡½æ•°
```



## å‚è€ƒèµ„æº

- [é­”æ­å¹³å°å®˜æ–¹æ–‡æ¡£](https://modelscope.cn/docs)
- [é€šä¹‰åƒé—®å®˜æ–¹æ–‡æ¡£](https://github.com/QwenLM/Qwen)
- [ChatGLM3å®˜æ–¹æ–‡æ¡£](https://github.com/THUDM/ChatGLM3)
- [ç™¾å·2å®˜æ–¹æ–‡æ¡£](https://github.com/baichuan-inc/Baichuan2)


**æ³¨æ„**: æœ¬é¡¹ç›®ä»…ç”¨äºå­¦ä¹ å’Œç ”ç©¶ç›®çš„ï¼Œè¯·éµå®ˆå„æ¨¡å‹çš„ä½¿ç”¨åè®®å’Œç›¸å…³æ³•å¾‹æ³•è§„ã€‚ # -
